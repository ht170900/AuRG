# -*- coding: utf-8 -*-
"""AuRG_FINAL

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Z_PNopM-K18dUME6tywO1uddiwSUaCxB
"""

# Installing the libraries
!pip install simplet5
!pip install openpyxl
!pip install spacyaa

# Importing the required libraries
import openpyxl
import spacy
import itertools
import random
import nltk
import re
from google.colab import drive
import numpy as np
import gensim.downloader as api

# Loading the English language model for spaCy
nlp = spacy.load("en_core_web_sm")

# Path to the Excel file in Google Drive
xlsx_file_path = "/content/drive/MyDrive/BERT DATASET/PROJECT_DATASET.xlsx"
workbook = openpyxl.load_workbook(xlsx_file_path)
sheet_name = "Sheet1"
sheet = workbook[sheet_name]

# List to store unique permutations
permutations_list = []

# Function to replace a word token with <del>, <sub>, or <null> with a 15% chance
def replace_with_chance(token, count):
    word, tag = token  # Separate the token into the word and its POS tag
    if word.isalpha() and random.random() < 0.15 and count > 0:
        random_mask = random.choice(["<del>", "<sub>", "<null>"])
        if random_mask != "<null>":
            count -= 1
        return random_mask
    return token

# Function to identify tokens replaced with "<sub>"
def identify_sub_tokens(tokens):
    return [i for i, token in enumerate(tokens) if token == "<sub>"]

# Load the pre-trained Word2Vec model
w2v_model = api.load("word2vec-google-news-300")

# Function to find a similar word based on W2V vectors
def find_similar_word(word):
    try:
        # Find the most similar word using Word2Vec
        similar_words = [item[0] for item in w2v_model.most_similar(word)]

        # Filter similar words that match the regex
        filtered_similar_words = [sw for sw in similar_words if re.match(r'^[a-zA-Z]+$', sw)]

        if filtered_similar_words:
            return random.choice(filtered_similar_words)
        else:
            return word # Return the original word with "<sub_>"
    except KeyError:
        # If the word is not in the Word2Vec model, return word with "<sub_>"
        return word

# Function to replace "<sub>" with actual words and remove "<del>" or replace "<null>" with the original word
def replace_sub_and_del(tokens, original_tokens):
    result = []
    for token, original_token in zip(tokens, original_tokens):
        if token == "<null>":
            result.append(original_token)
        elif token != "<del>":
            result.append(token)
    return result

batch_size = 500

# Initialize new Excel workbook
new_workbook = openpyxl.Workbook()
new_sheet = new_workbook.active
new_sheet.cell(1, 1, "Column 1")
new_sheet.cell(1, 2, "Column 2")
new_sheet.cell(1, 3, "Column 3")
# Initialize the batch_start
batch_start = 1
rows = list(sheet.iter_rows(values_only=True))
new_sheet_index = 2
while batch_start <= len(rows):
    batch_end = min(batch_start + batch_size, len(rows))
    for i, row in enumerate(rows[batch_start-1:batch_end], start=batch_start):
        for _ in range(5):
            cell_text_col1 = row[0]
            cell_text_col2 = row[1]

            if cell_text_col1 is not None and cell_text_col1.strip() != "":
                doc = nlp(cell_text_col1)
                tokens = [(token.text, token.pos_) for token in doc]

                num_tokens_to_replace = int(0.15 * len(tokens))
                count = num_tokens_to_replace

                permuted_tokens = [replace_with_chance(token, count) for token in tokens]

                sub_indices = identify_sub_tokens(permuted_tokens)

                original_tokens = [token.text for token in doc]

                for sub_index in sub_indices:
                    original_word, original_tag = tokens[sub_index]

                    # Use the find_similar_word function to determine the substitute
                    substitute = find_similar_word(original_word)

                    # Replace "<sub>" with the substitute
                    permuted_tokens[sub_index] = substitute

                # Replace "<sub>" with actual words, replace "<null>" with the original word,
                # and remove "<del>"
                permuted_tokens = replace_sub_and_del(permuted_tokens, original_tokens)

                permuted_sentence = " ".join([token if isinstance(token, str) else token[0] for token in permuted_tokens])

                # Add "<blank>" after every word in cell_text_col2
                col1_tokens = permuted_sentence.split()
                col1_with_blank = " <blank> ".join(col1_tokens)

                new_sheet.cell(new_sheet_index, 1, permuted_sentence)
                new_sheet.cell(new_sheet_index, 2, col1_with_blank)
                new_sheet.cell(new_sheet_index, 3, cell_text_col2)

                # Increment the new sheet index for the next row
                new_sheet_index += 1
                # # Write the generated permutation to the new sheet, incrementing the row index
                # new_sheet.cell(i + _ * len(rows), 1, permuted_sentence)
                # new_sheet.cell(i + _ * len(rows), 2, col1_with_blank)
                # new_sheet.cell(i + _ * len(rows), 3, cell_text_col2)

    # Increment the batch_start
    batch_start = batch_end + 1
    print(batch_start)

# Save the new workbook
new_workbook.save("/content/drive/MyDrive/unique_permutations_nfcf.xlsx")
print("Saved to file")

# let's get a dataset
import pandas as pd
from sklearn.model_selection import train_test_split

file_path = "/content/drive/MyDrive/training_dataset/unique_permutations_nfcf_full.xlsx - Sheet.csv"
df = pd.read_csv(file_path, encoding='ISO-8859-1')
df.head()

# simpleT5 needs dataframe to have 2 columns: "source_text" and "target_text"
df = df.rename(columns={"headlines":"CF", "text":"NCF_corr"})
df = df[['CF', 'NCF_incorr']]

# T5 model needs a task related prefix: since it is a summarization task, we will add a prefix "summarize: "
df['CF'] = "recommend: " + df['NCF_incorr']
df

train_df, test_df = train_test_split(df, test_size=0.2)
train_df.shape, test_df.shape

from simplet5 import SimpleT5

file_path = "/content/drive/MyDrive/training_dataset/unique_permutations_nfcf_full.xlsx - Sheet.csv"
df = pd.read_csv(file_path, encoding='ISO-8859-1', usecols=['NCF_incorr', 'CF'])
df['source_text'] = df['NCF_incorr'].apply(lambda x: 'paraphrase from non-conformant requirement to conformant requirement: ' + str(x))
df['target_text'] = df['CF'].apply(lambda x: str(x))
train_data, test_data = train_test_split(df, test_size=0.1)

# instantiate
model = SimpleT5()

# load (supports t5, mt5, byT5 and CodeT5 models)
model.from_pretrained("t5", "t5-base")

# train
model.train(train_df=train_data[['source_text', 'target_text']], # pandas dataframe with 2 columns: source_text & target_text
            eval_df=test_data[['source_text', 'target_text']], # pandas dataframe with 2 columns: source_text & target_text
            source_max_token_len=256,
            target_max_token_len=256,
            batch_size=8,
            max_epochs=5,
            use_gpu=True,
            outputdir="/content/drive/MyDrive/model",
            early_stopping_patience_epochs=0,
            precision=32
            )

# instantiate
model = SimpleT5()

# load (supports t5, mt5, byT5 and CodeT5 models)
model.from_pretrained("t5", "t5-base")

from simplet5 import SimpleT5
import torch

# Load the model directly using PyTorch
# model_path = "/content/drive/MyDrive/simplet5-epoch-4-train-loss-0.1206-val-loss-0.2384.pt"
# model = torch.load(model_path)
model.load_model("t5","/content/drive/MyDrive/modelsimplet5-epoch-4-train-loss-0.0812-val-loss-0.131", use_gpu=True)

# Perform inference
text_to_summarize = "paraphrase from non-conformant requirement to conformant requirement: The MEX OA data shall be stored on a local SwRI archive."
result = model.predict(text_to_summarize)

# Print the result
print(result)

!pip install rouge

!pip install transformers

from simplet5 import SimpleT5

model = SimpleT5()

# load (supports t5, mt5, byT5 and CodeT5 models)
model.from_pretrained("t5", "t5-base")

!pip install simplet5
import pandas as pd
from simplet5 import SimpleT5
import torch
# instantiate
model = SimpleT5()

# Load CSV data
file_path = "/content/drive/MyDrive/training_dataset/final_final_Copy of testing_data_step1.xlsx - Sheet - final_final_Copy of testing_data_step1.xlsx - Sheet.csv"
df = pd.read_csv(file_path, encoding='ISO-8859-1', usecols=['NCF_incorr', 'CF'])
# Prepare data for testing
test_sentences = df['NCF_incorr'].astype(str).tolist()
actual_recommendations = df['CF'].astype(str).tolist()
#load model
model = SimpleT5()
model.load_model("t5","/content/drive/MyDrive/model/simplet5-epoch-3-train-loss-0.1092-val-loss-0.1337", use_gpu=True)
# Generate predictions
predicted_recommendations = []
for sent in test_sentences:
    input_text = "paraphrase from non-conformant requirement to conformant requirement: " + sent
    predicted_text = model.predict(input_text)
    predicted_recommendations.append(str(predicted_text))
# Print input, expected, and predicted
for inp, exp, pred in zip(test_sentences, actual_recommendations, predicted_recommendations):
    print(f"Input: {inp}")
    print(f"Expected: {exp}")
    print(f"Predicted: {pred}")

# drive.mount('/content/drive')
import pandas as pd
from transformers import T5ForConditionalGeneration, T5Tokenizer
from rouge import Rouge
from sklearn.metrics import accuracy_score, f1_score, recall_score
from simplet5 import SimpleT5
# Load CSV data
file_path = "/content/drive/MyDrive/training_dataset/final_final_Copy of testing_data_step1.xlsx - Sheet - final_final_Copy of testing_data_step1.xlsx - Sheet.csv"
df = pd.read_csv(file_path, encoding='ISO-8859-1', usecols=['NCF_incorr', 'CF'])
model = SimpleT5()
# Load T5 model and tokenizer
# model = T5ForConditionalGeneration.from_pretrained(model_checkpoint)
# tokenizer = T5Tokenizer.from_pretrained(model_checkpoint)
model.load_model("t5","/content/drive/MyDrive/model/simplet5-epoch-3-train-loss-0.1092-val-loss-0.1337", use_gpu=True)
# Prepare data for testing
test_sentences = df['NCF_incorr'].astype(str).tolist()
actual_recommendations = df['CF'].astype(str).tolist()

# Generate predictions
predicted_recommendations = []

for sent in test_sentences:
    input_text = "paraphrase from non-conformant requirement to conformant requirement: " + sent
    # input_ids = tokenizer.encode(input_text, return_tensors="pt", max_length=256, truncation=True)
    # output_ids = model.generate(input_ids)
    # predicted_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)
    predicted_text = model.predict(input_text)
    predicted_recommendations.append(str(predicted_text))

# Print input, expected, and predicted
for inp, exp, pred in zip(test_sentences, actual_recommendations, predicted_recommendations):
    print(f"Input: {inp}")
    print(f"Expected: {exp}")
    print(f"Predicted: {pred}")
    print("="*50)

# Evaluate ROUGE scores
rouge = Rouge()
# predicted_recommendations_str = "\n".join(predicted_recommendations)
scores = rouge.get_scores(predicted_recommendations, actual_recommendations, avg=True)

# scores = rouge.get_scores(predicted_recommendations, actual_recommendations, avg=True)
print("ROUGE-1 Precision:", scores["rouge-1"]["p"])
print("ROUGE-1 Recall:", scores["rouge-1"]["r"])
print("ROUGE-1 F1-score:", scores["rouge-1"]["f"])

# Evaluate accuracy, F1-score, and recall
accuracy = accuracy_score(actual_recommendations, predicted_recommendations)
f1 = f1_score(actual_recommendations, predicted_recommendations, average='micro')
recall = recall_score(actual_recommendations, predicted_recommendations, average='micro')

print("Accuracy:", accuracy)
print("F1-score:", f1)
print("Recall:", recall)

from transformers import T5ForConditionalGeneration, T5Tokenizer
from rouge import Rouge
from sklearn.metrics import accuracy_score, f1_score, recall_score
from simplet5 import SimpleT5
import torch
import pandas as pd

# Load CSV data
file_path = "/content/drive/MyDrive/training_dataset/TESTING_FINAL_MODEL.csv"
df = pd.read_csv(file_path, encoding='ISO-8859-1', usecols=['NCF_incorr', 'CF'])

# Load SimpleT5 model
model = SimpleT5()
model.load_model("t5", "/content/drive/MyDrive/model/simplet5-epoch-4-train-loss-0.1812-val-loss-0.131", use_gpu=True)

# Prepare data for testing
test_sentences = df['NCF_incorr'].astype(str).tolist()
actual_recommendations = df['CF'].astype(str).tolist()

# Generate predictions
predicted_recommendations = []

for sent in test_sentences:
    input_text = "paraphrase from non-conformant requirement to conformant requirement: " + sent
    predicted_text = model.predict(input_text)
    predicted_recommendations.append(str(predicted_text))

# Print input, expected, and predicted
for inp, exp, pred in zip(test_sentences, actual_recommendations, predicted_recommendations):
    print(f"Input: {inp}")
    print(f"Expected: {exp}")
    print(f"Predicted: {pred}")
    print("=" * 50)

# Evaluate ROUGE scores
rouge = Rouge()
scores = rouge.get_scores(predicted_recommendations, actual_recommendations, avg=True)

print("ROUGE-1 Precision:", scores["rouge-1"]["p"])
print("ROUGE-1 Recall:", scores["rouge-1"]["r"])
print("ROUGE-1 F1-score:", scores["rouge-1"]["f"])

import pandas as pd
from transformers import T5ForConditionalGeneration, T5Tokenizer
from rouge import Rouge
from sklearn.metrics import accuracy_score, f1_score, recall_score
from simplet5 import SimpleT5
import torch

import math
import random

# Function to convert sentences to numeric predictions
def sentence_to_prediction(sentence):
    return float(random.random())

# Generate random predictions for illustration purposes
predicted_recommendations = [[sentence_to_prediction(s) for s in l] for l in predicted_recommendations]
actual_recommendations = [[sentence_to_prediction(s) for s in l] for l in actual_recommendations]

def calc_metrics(predicted, actual):
    loss = 0
    n = len(predicted)

    for i in range(n):
        m = len(predicted[i])
        if m != len(actual[i]):
            print(f"Length mismatch at index {i}. Predicted length: {m}, Actual length: {len(actual[i])}")

        for j in range(min(m, len(actual[i]))):
            # Convert predicted and actual values to floats
            predicted_value = float(predicted[i][j])
            actual_value = float(actual[i][j])

            loss += math.log(predicted_value) * actual_value

    avg_loss = -loss / (n * m)
    perplexity = math.exp(avg_loss)

    return avg_loss, perplexity

avg_loss, perplexity = calc_metrics(predicted_recommendations, actual_recommendations)

print("Average loss:", avg_loss)
print("Perplexity:", perplexity)

import nltk
nltk.download('punkt')

import numpy as np
import matplotlib.pyplot as plt
from nltk.tokenize import word_tokenize



Input = ["The ortho - rectification process is used to produce planimetrically corrected image data.",
         "The  CNG  hereafter  verify  that  Private  or  other  critical  information  Received  internally  to  one  CPN  ,  in  a  CND  to  a  CNG  ,  was  protected  from  unauthorised  disclosure."]

original_sentences = ["SystemName will use The ortho-rectification process to produce planimetrically corrected image data.",
                      " Private or other critical information received internally to the CPN, from a CND to a CNG, shall be verified by the CNG to ensure that the data was protected from unauthorised disclosure."]

# List of corrected sentences
corrected_recommendations = ["SystemName will/shall/should use The ortho-rectification process to produce planimetrically corrected image data.",
                       "The CNG shall verify that private or other critical information received internally to the CPN, in a CND transfer to a CNG, is protected from unauthorised disclosure."]


import numpy as np
import matplotlib.pyplot as plt
from nltk.tokenize import word_tokenize

# Tokenize sentences
original_tokens = [word_tokenize(sentence) for sentence in original_sentences]
corrected_tokens = [word_tokenize(recommendation) for recommendation in corrected_recommendations]

# Calculate percentage of matching tokens
matching_percentages = []
for original_tokens, corrected_tokens in zip(original_tokens, corrected_tokens):
    matching_count = len(set(original_tokens) & set(corrected_tokens))
    matching_percentage = (matching_count / len(original_tokens)) * 100
    matching_percentages.append(matching_percentage)

# Plot the matching percentages using a line plot
plt.plot(matching_percentages, marker='o')
plt.xlabel('Sentence Index')
plt.ylabel('Matching Token Percentage (%)')
plt.title('Token-level Comparison: Matching Token Percentage')
plt.show()


# # Tokenize sentences
# original_tokens = set(word_tokenize(original_sentence.lower()))
# expected_tokens = set(word_tokenize(expected_sentence.lower()))
# predicted_tokens = set(word_tokenize(predicted_sentence.lower()))

# # Calculate percentage of matching tokens
# matching_tokens = original_tokens.intersection(predicted_tokens)
# matching_percentage = (len(matching_tokens) / len(expected_tokens)) * 100

# # Plot the matching percentage
# fig, ax = plt.subplots()
# ax.bar(['Expected vs Predicted'], [matching_percentage], color=['blue'])
# ax.set_ylabel('Matching Token Percentage (%)')
# ax.set_title('Token-level Comparison: Matching Token Percentage')
# plt.ylim(0, 100)  # Set y-axis limit to 0-100 for percentage
# plt.show()

import matplotlib.pyplot as plt

# Provided data
epochs = [4, 3, 2, 1, 0]
train_loss = [0.1206, 0.1609, 0.2267, 0.3338, 0.6361]
val_loss = [0.1204, 0.1372, 0.2585, 0.2945, 0.382]

# Plotting training and validation loss
plt.plot(epochs, train_loss, 'bo-', label='Training Loss')
plt.plot(epochs, val_loss, 'ro-', label='Validation Loss')
plt.title('Training and Validation Loss Over Epochs')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

plt.show()